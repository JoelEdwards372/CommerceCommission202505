{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# 01 - Acquire Packages For Data Preparation Work\n",
                "\n",
                "Includes:\n",
                "- loading required PACKAGES\n",
                "- provided DIRECTORY for filepaths and github/version control functionality for structuring data and frameworks (not fully used as had limited time)\n",
                "- FUNCTIONS (which would have been encapsulated within classes and appropriate github/version control directories - but with limited time only included here).\n",
                "  - A data type/preparation function (TRANSFORMATION - fn_transform_cast)\n",
                "  - A function which creates dimensional tables (TRANSFORMATION for Star Schemas - fn_create_dim_table)\n",
                "  - A function that creates spark dataframes based on user suggestions (EXTRACTION/LOAD - fn_dataframe_selections)\n",
                "  - A function that extends/creates attributes relevant to dates (TRANSFORMATION - fn_add_period_attributes)\n",
                "  - Functions include testing, error checking, correction, and validation"
            ],
            "metadata": {
                "id": "16-u96m-FmNc"
            }
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "id": "b17VQJqhQ5BT",
                "outputId": "6bd5b719-6d58-4a34-a97c-b8f0d7bffffb",
                "colab": {
                    "base_uri": "https://localhost:8080/"
                }
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
                        "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
                        "Requirement already satisfied: ydata-profiling==4.6.4 in /usr/local/lib/python3.11/dist-packages (4.6.4)\n",
                        "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
                        "Requirement already satisfied: scipy<1.12,>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ydata-profiling==4.6.4) (1.11.4)\n",
                        "Requirement already satisfied: matplotlib<3.9,>=3.2 in /usr/local/lib/python3.11/dist-packages (from ydata-profiling==4.6.4) (3.8.4)\n",
                        "Requirement already satisfied: pydantic>=2 in /usr/local/lib/python3.11/dist-packages (from ydata-profiling==4.6.4) (2.11.4)\n",
                        "Requirement already satisfied: PyYAML<6.1,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from ydata-profiling==4.6.4) (6.0.2)\n",
                        "Requirement already satisfied: jinja2<3.2,>=2.11.1 in /usr/local/lib/python3.11/dist-packages (from ydata-profiling==4.6.4) (3.1.6)\n",
                        "Requirement already satisfied: visions==0.7.5 in /usr/local/lib/python3.11/dist-packages (from visions[type_image_path]==0.7.5->ydata-profiling==4.6.4) (0.7.5)\n",
                        "Requirement already satisfied: numpy<1.26,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from ydata-profiling==4.6.4) (1.25.2)\n",
                        "Requirement already satisfied: htmlmin==0.1.12 in /usr/local/lib/python3.11/dist-packages (from ydata-profiling==4.6.4) (0.1.12)\n",
                        "Requirement already satisfied: phik<0.13,>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from ydata-profiling==4.6.4) (0.12.4)\n",
                        "Requirement already satisfied: requests<3,>=2.24.0 in /usr/local/lib/python3.11/dist-packages (from ydata-profiling==4.6.4) (2.32.3)\n",
                        "Requirement already satisfied: tqdm<5,>=4.48.2 in /usr/local/lib/python3.11/dist-packages (from ydata-profiling==4.6.4) (4.67.1)\n",
                        "Requirement already satisfied: seaborn<0.13,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from ydata-profiling==4.6.4) (0.12.2)\n",
                        "Requirement already satisfied: multimethod<2,>=1.4 in /usr/local/lib/python3.11/dist-packages (from ydata-profiling==4.6.4) (1.12)\n",
                        "Requirement already satisfied: statsmodels<1,>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from ydata-profiling==4.6.4) (0.14.4)\n",
                        "Requirement already satisfied: typeguard<5,>=4.1.2 in /usr/local/lib/python3.11/dist-packages (from ydata-profiling==4.6.4) (4.4.2)\n",
                        "Requirement already satisfied: imagehash==4.3.1 in /usr/local/lib/python3.11/dist-packages (from ydata-profiling==4.6.4) (4.3.1)\n",
                        "Requirement already satisfied: wordcloud>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from ydata-profiling==4.6.4) (1.9.4)\n",
                        "Requirement already satisfied: dacite>=1.8 in /usr/local/lib/python3.11/dist-packages (from ydata-profiling==4.6.4) (1.9.2)\n",
                        "Requirement already satisfied: numba<0.59.0,>=0.56.0 in /usr/local/lib/python3.11/dist-packages (from ydata-profiling==4.6.4) (0.58.1)\n",
                        "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.11/dist-packages (from imagehash==4.3.1->ydata-profiling==4.6.4) (1.8.0)\n",
                        "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from imagehash==4.3.1->ydata-profiling==4.6.4) (11.2.1)\n",
                        "Requirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.11/dist-packages (from visions==0.7.5->visions[type_image_path]==0.7.5->ydata-profiling==4.6.4) (25.3.0)\n",
                        "Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.11/dist-packages (from visions==0.7.5->visions[type_image_path]==0.7.5->ydata-profiling==4.6.4) (3.4.2)\n",
                        "Requirement already satisfied: tangled-up-in-unicode>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from visions==0.7.5->visions[type_image_path]==0.7.5->ydata-profiling==4.6.4) (0.2.0)\n",
                        "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
                        "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
                        "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
                        "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2<3.2,>=2.11.1->ydata-profiling==4.6.4) (3.0.2)\n",
                        "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.9,>=3.2->ydata-profiling==4.6.4) (1.3.2)\n",
                        "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.9,>=3.2->ydata-profiling==4.6.4) (0.12.1)\n",
                        "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.9,>=3.2->ydata-profiling==4.6.4) (4.58.0)\n",
                        "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.9,>=3.2->ydata-profiling==4.6.4) (1.4.8)\n",
                        "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.9,>=3.2->ydata-profiling==4.6.4) (24.2)\n",
                        "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.9,>=3.2->ydata-profiling==4.6.4) (3.2.3)\n",
                        "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba<0.59.0,>=0.56.0->ydata-profiling==4.6.4) (0.41.1)\n",
                        "Requirement already satisfied: joblib>=0.14.1 in /usr/local/lib/python3.11/dist-packages (from phik<0.13,>=0.11.1->ydata-profiling==4.6.4) (1.5.0)\n",
                        "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->ydata-profiling==4.6.4) (0.7.0)\n",
                        "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->ydata-profiling==4.6.4) (2.33.2)\n",
                        "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->ydata-profiling==4.6.4) (4.13.2)\n",
                        "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->ydata-profiling==4.6.4) (0.4.0)\n",
                        "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
                        "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.24.0->ydata-profiling==4.6.4) (3.4.2)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.24.0->ydata-profiling==4.6.4) (3.10)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.24.0->ydata-profiling==4.6.4) (2.4.0)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.24.0->ydata-profiling==4.6.4) (2025.4.26)\n",
                        "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels<1,>=0.13.2->ydata-profiling==4.6.4) (1.0.1)\n",
                        "fatal: destination path 'CommerceCommission202505' already exists and is not an empty directory.\n"
                    ]
                }
            ],
            "source": [
                "#PACKAGES\n",
                "!pip install pyspark\n",
                "!pip install ydata-profiling==4.6.4 pandas\n",
                "\n",
                "# CLONE REPOSITORY\n",
                "!git clone https://github.com/JoelEdwards372/CommerceCommission202505.git\n",
                "\n",
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql.types import StructType, StructField, StringType, DateType, TimestampType\n",
                "from pyspark.sql.functions import col, to_date, when, regexp_replace, year, quarter, concat, lit, when, month, ceil\n",
                "from ydata_profiling import ProfileReport\n",
                "import os # Import os module for path manipulation\n",
                "import shutil # Integration with version control platforms (github, gitlab, etc.)\n",
                "import pandas as pd # Import pandas\n",
                "import datetime\n",
                "\n",
                "\n",
                "# DIRECTORIES\n",
                "# These clone/copy github repositories, and link to filepaths to either SOURCE or OUTPUT data\n",
                "# Ensure this path is correct and the file exists at this location\n",
                "fpath_tenancy_data = '/content/CommerceCommission202505/src_data/Detailed-Monthly-Region-Tenancy.csv'\n",
                "fpath_data_quality_profile = '/content/CommerceCommission202505/data_quality_profiles/'\n",
                "fpath_data_star_schema = '/content/CommerceCommission202505/data_star_schema_prep/'\n",
                "\n",
                "# FUNCTIONS (Placed in classes in the ETL development directory - but have retained here in notebook)\n",
                "# THE CLASS CONTAINING FUNCTIONS WOULD HAVE BEEN imported via .... \"from etl_development.etl.joels_etl_class import JoelsETL\"\n",
                "# THEN INSTANTIATED AS A ETL class ... \"joels_etl = JoelsETL(spark)\" ... with functions called via joels_etl.fn_transform_cast etc...\n",
                "\n",
                "# Function that explicitly TRANSFORMS string type fields that are either date, integer, or double type\n",
                "def fn_transform_cast(df, columns, cast_type):\n",
                "    \"\"\"\n",
                "    Objective:\n",
                "        Casts multiple string columns to a specified data type in a PySpark DataFrame.\n",
                "\n",
                "    Args:\n",
                "        df (DataFrame): The input PySpark DataFrame.\n",
                "        columns (list of str): A list of column names to be cast.\n",
                "        cast_type (str): The target data type for casting (\"date\", \"integer\", or \"double\").\n",
                "\n",
                "    Returns:\n",
                "        DataFrame: A new DataFrame with specified columns cast to the target type.\n",
                "\n",
                "    Use Case:\n",
                "    i.e. fn_transform_cast(df, [\"col1\", \"col2\"], \"integer\")\n",
                "    i.e. fn_transform_cast(df, [\"col3\"], \"date\")\n",
                "    \"\"\"\n",
                "    valid_cast_types = [\"date\", \"integer\", \"double\"]\n",
                "    if cast_type.lower() not in valid_cast_types:\n",
                "        raise ValueError(f\"Invalid cast_type. Supported types are: {valid_cast_types}\")\n",
                "\n",
                "    for column in columns:\n",
                "        new_col_name = f\"tfm_{column}\"\n",
                "        if cast_type.lower() == \"date\":\n",
                "            df = df.withColumn(new_col_name, col(column).cast(\"date\"))\n",
                "        else:\n",
                "            # For integer and double, remove commas before casting\n",
                "            df = df.withColumn(new_col_name, regexp_replace(col(column), \",\", \"\").cast(cast_type))\n",
                "    return df\n",
                "\n",
                "\n",
                "# Function to TRANSFORM tables into dimensional tables\n",
                "def fn_create_dim_table(df_input, dimension_cols, order_by_col=None):\n",
                "    \"\"\"\n",
                "    Objective:\n",
                "        Creates a dimension table from a Spark DataFrame based on specified columns.\n",
                "\n",
                "    Args:\n",
                "        df_input (DataFrame): The input PySpark DataFrame with transformed columns.\n",
                "        dimension_cols (list of str): A list of column names to be included\n",
                "                                     in the dimension table.\n",
                "        order_by_col (str, optional): The name of the column to order the dimension\n",
                "                                      table by. If None, the first column in\n",
                "                                      dimension_cols is used for ordering.\n",
                "                                      Defaults to None.\n",
                "\n",
                "    Returns:\n",
                "        DataFrame: The created PySpark dimension table.\n",
                "\n",
                "    Use Case (Creating Location Dimension):\n",
                "    df_dim_location = create_dimension_table(\n",
                "        df_spark_tfm,\n",
                "        [\"tfm_Location Id\", \"Location\"],\n",
                "        \"tfm_Location Id\" # Optional: explicitly specify the order column\n",
                "    )\n",
                "\n",
                "    Use Case (Creating Period Dimension - basic):\n",
                "    df_dim_period_basic = create_dimension_table(\n",
                "        df_spark_tfm,\n",
                "        [\"tfm_Time Frame\"]\n",
                "    )\n",
                "    \"\"\"\n",
                "    # Validate input columns\n",
                "    if not isinstance(dimension_cols, list) or not dimension_cols:\n",
                "        raise ValueError(\"dimension_cols must be a non-empty list of column names.\")\n",
                "\n",
                "    if not all(c in df_input.columns for c in dimension_cols):\n",
                "        missing_cols = [c for c in dimension_cols if c not in df_input.columns]\n",
                "        raise ValueError(f\"Input DataFrame is missing required columns: {missing_cols}\")\n",
                "\n",
                "    # Determine the column for ordering\n",
                "    if order_by_col is None:\n",
                "        order_col = dimension_cols[0]\n",
                "    else:\n",
                "        if order_by_col not in dimension_cols:\n",
                "             raise ValueError(f\"order_by_col '{order_by_col}' must be one of the columns in dimension_cols.\")\n",
                "        order_col = order_by_col\n",
                "\n",
                "    # Create the dimension table\n",
                "    df_dimension = (df_input.\n",
                "                    select(dimension_cols).\n",
                "                    distinct().\n",
                "                    orderBy(order_col))\n",
                "\n",
                "    return df_dimension\n",
                "\n",
                "\n",
                "# Function that creates new dataframes, accepting user field selections\n",
                "def fn_dataframe_selections(df_spark, selected_fields):\n",
                "  \"\"\"\n",
                "  Selects and reorders a subset of columns in a Spark DataFrame\n",
                "  according to a provided list.\n",
                "\n",
                "  Args:\n",
                "    df_spark (DataFrame): The input PySpark DataFrame.\n",
                "    selected_fields (list of str): A list of column names\n",
                "                                          specifying the subset of columns\n",
                "                                          to select and their desired order.\n",
                "                                          These column names should match\n",
                "                                          names present in the input DataFrame.\n",
                "\n",
                "  Returns:\n",
                "    DataFrame: A new DataFrame with the specified columns in the desired order.\n",
                "               Returns None if selected_fields is empty or not a list.\n",
                "\n",
                "  Raises:\n",
                "      ValueError: If any column in desired_subset_order is not found\n",
                "                  in the input DataFrame.\n",
                "\n",
                "  Use Cases (including testing):\n",
                "\tExample of how to use the function:\n",
                "\tAssuming df_spark is your Spark DataFrame\n",
                "\n",
                "\tExample 1: Select and reorder a few columns\n",
                "\tselected_fields = [\"tfm_Time Frame\", \"Location\", \"tfm_Median Rent\"]\n",
                "\tdf_subset1 = fn_dataframe_selections(df_spark, selected_fields)\n",
                "\tif df_subset1 is not None:\n",
                "\t\tdf_subset1.show()\n",
                "\n",
                "\tExample 2: Select and reorder almost all columns in a specific order\n",
                "\tselected_fields = [\"tfm_Location Id\", \"Location\", \"tfm_Time Frame\", \"tfm_Lodged Bonds\",\n",
                "\t\"tfm_Active Bonds\", \"tfm_Closed Bonds\", \"tfm_Median Rent\"]\n",
                "\tdf_subset2 = fn_dataframe_selections(df_spark, selected_fields)\n",
                "\tif df_subset2 is not None:\n",
                "\t\tdf_subset2.show()\n",
                "\n",
                "\tExample 3: Handling an invalid column (will raise ValueError)\n",
                "\ttry:\n",
                "\t\tselected_fields = [\"tfm_Time Frame\", \"NonExistentColumn\"]\n",
                "\t\tdf_invalid = fn_dataframe_selections(df_spark, selected_fields)\n",
                "\texcept ValueError as e:\n",
                "\t\tprint(f\"Caught expected error: {e}\")\n",
                "  \"\"\"\n",
                "  if not isinstance(selected_fields, list) or not selected_fields:\n",
                "      print(\"Error: desired_subset_order must be a non-empty list of column names.\")\n",
                "      return None\n",
                "\n",
                "  # Check if all requested columns exist in the DataFrame\n",
                "  missing_cols = [col_name for col_name in selected_fields if col_name not in df_spark.columns]\n",
                "  if missing_cols:\n",
                "      raise ValueError(f\"The following requested columns are not found in the DataFrame: {missing_cols}\")\n",
                "\n",
                "  # Select and reorder the columns\n",
                "  return df_spark.select(selected_fields)\n",
                "\n",
                "\n",
                "# Function that expands of data/time fields for dim date relevant tables\n",
                "def fn_add_period_attributes(df_spark, date_column):\n",
                "  \"\"\"\n",
                "  Adds Year, Annual Quarter, and Financial Quarter attributes to a Spark DataFrame\n",
                "  based on a specified date column.\n",
                "\n",
                "  Args:\n",
                "    df_spark (DataFrame): The input PySpark DataFrame.\n",
                "    date_column (str): The name of the date column from which to calculate the attributes.\n",
                "                       This column should be of a date or timestamp type.\n",
                "\n",
                "  Returns:\n",
                "    DataFrame: A new DataFrame with the added period attributes.\n",
                "\n",
                "  Raises:\n",
                "    ValueError: If the specified date_column is not found in the DataFrame\n",
                "                or is not of a date/timestamp type.\n",
                "\n",
                "  Use Cases:\n",
                "  # Assuming df_dim_period is your Spark DataFrame and 'tfm_Time Frame' would be the date column\n",
                "  # df_dim_period_with_attributes = add_period_attributes(df_dim_period, \"tfm_Time Frame\")\n",
                "  \"\"\"\n",
                "  if date_column not in df_spark.columns:\n",
                "      raise ValueError(f\"The specified date column '{date_column}' is not found in the DataFrame.\")\n",
                "\n",
                "  # Check if the column is a date or timestamp type\n",
                "  column_type = df_spark.schema[date_column].dataType\n",
                "  if not (isinstance(column_type, DateType) or isinstance(column_type, TimestampType)):\n",
                "       raise ValueError(f\"The column '{date_column}' is not a date or timestamp type. \"\n",
                "                        f\"Current type is {column_type}.\")\n",
                "\n",
                "  df_with_attributes = (df_spark.\n",
                "                 withColumn(\"Year\", year(col(date_column))).\n",
                "                 withColumn(\"Annual Quarter\", concat(year(col(date_column)), lit(\"Q\"), quarter(col(date_column)))).\n",
                "                 withColumn(\"Financial Quarter\",\n",
                "                            concat(\n",
                "                                when(month(col(date_column)) >= 7, year(col(date_column)))\n",
                "                                .otherwise(year(col(date_column)) - 1),\n",
                "                                lit(\"Q\"),\n",
                "                                when(month(col(date_column)) >= 7, ceil((month(col(date_column)) - 6) / 3))\n",
                "                                .otherwise(ceil((month(col(date_column)) + 6) / 3))\n",
                "                            )\n",
                "                 ))\n",
                "  return df_with_attributes"
            ]
        },
        {
            "cell_type": "markdown",
            "source": [
                "# 02 - Setup Spark to ingest and ETL non-geospatial data\n",
                "Includes:\n",
                "- creating a spark session\n",
                "- accessing tenancy data from original file with ...\n",
                "  - a pre-formed schema (non-parsing) to EXTRACT data (as is)\n",
                "  - basic TRANSFORMATION/PREPARATION of non-geospatial datasets\n",
                "  - examination of data quality leveraging YData Quality Framework tool (and saving to \"data quality profile\" folder in interactive html format."
            ],
            "metadata": {
                "id": "8c9wkSgyF0JY"
            }
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {
                "id": "SAO_lPfxi2K1",
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 1000,
                    "referenced_widgets": [
                        "bc539bf396ea4f418e9e052e8bf7c322",
                        "fc25d1f35725464e96a67c10a6d15986",
                        "8b183f8ea6da4343a8d00a48e7ddf273",
                        "2a89503333964544850e313aae67c1bc",
                        "d8d04d9576e74ecabf060f3b151075c3",
                        "2bbc98591a7c4ba4b2866dc8af3e723a",
                        "d1b6a5c3efd24aae8319719912a4e908",
                        "d56d76107fc9456b86cfc62628e87f46",
                        "816dd7989cb94303b3e894335b11b13b",
                        "4b63fdf6a74e49cba37ddfd559f0c347",
                        "7d6ce578b406438d8d5b8f3895d6238a",
                        "e5f9fabaf06b4d9bb6bb391236d0f502",
                        "117052f9b8324eebb9a9046c1e96bf70",
                        "a9c736c62a9449c2a1d96d01a5a667ad",
                        "2b52e379555e4f70a0bdee1128bb2b3c",
                        "b7e6b8b146934e9b831e90e77293df83",
                        "ee588653e7d246c3ac739614b5104a39",
                        "db06d043f32c4a598f1d34f1de9cd38f",
                        "8c8fbfa9cb234f2188a4ebe77a468efe",
                        "10cd237d5cdc4ee98e0be3897b82f9f9",
                        "965622a868844f26a684bf36bdc31d0c",
                        "fdc8236eac2f4baea2ad1348d3e6edb8",
                        "58f4d8b5e02d4871901de69b82d1cc5f",
                        "4cfe85f703ca41b198f8f385f8d1320f",
                        "8379b1afdaa64201ab3f1f96393313b2",
                        "7c48c8820161440799f20e8144adf749",
                        "c326b82125bd40eaa16db0e4649aaeb1",
                        "3c65ea8c3b644ce9b55aaaadfc6c4454",
                        "7b6357ad45ec41b69901ad8234097a77",
                        "ec4bdce69d5547c8a2f4c9bf6f694c4c",
                        "c34958f3d9ba47969899619f089aeb85",
                        "fea3da3820fb43f880c50e9a0dca176d",
                        "0be4e9316c934f4098fcafaf7d3cf532",
                        "f4426fc8044c4e1ab06a20967d3bc0ae",
                        "04bbf102513f4d56a1451a4ba8b7a41b",
                        "a8cf3df3f1164ce58570571fcdc780c4",
                        "45e8f12035d044299bde340a764d3eb4",
                        "5b128a5d44d74e54b4f77f89f4519803",
                        "40d833634a87418db98313ce3693b9e2",
                        "81952d99281b4e31a30ca4b67de519eb",
                        "eb6b1e24109c4a1aa4ed0c97a8617015",
                        "8573d3fa96374397ae022411c196066a",
                        "e7edf0bc7f3143678e6f741e545ee6f3",
                        "d01480dd609146b7a5d3a1074d8dcc68"
                    ]
                },
                "outputId": "c71557c0-6c51-4afe-db69-56ad55f89b64"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Attempting to read data from local file: /content/CommerceCommission202505/src_data/Detailed-Monthly-Region-Tenancy.csv\n",
                        "DataFrame head:\n",
                        "+----------+-----------+--------------------+------------+------------+------------+-----------+-------------------+-------------------+-------------------+-----------------------+\n",
                        "|Time Frame|Location Id|            Location|Lodged Bonds|Active Bonds|Closed Bonds|Median Rent|Geometric Mean Rent|Upper Quartile Rent|Lower Quartile Rent|Log Std Dev Weekly Rent|\n",
                        "+----------+-----------+--------------------+------------+------------+------------+-----------+-------------------+-------------------+-------------------+-----------------------+\n",
                        "|1993-02-01|        -99|                 ALL|       9,144|       91635|       7,119|        150|                151|                200|                120|                   0.43|\n",
                        "|1993-02-01|         -1|                  NA|         390|        4932|         336|        130|                128|                165|                100|                   0.46|\n",
                        "|1993-02-01|          1|    Northland Region|         177|        1770|         141|        120|                119|                146|                100|                   0.30|\n",
                        "|1993-02-01|          2|     Auckland Region|       2,379|       29037|       1,965|        180|                186|                220|                150|                   0.36|\n",
                        "|1993-02-01|          3|      Waikato Region|         906|        8304|         693|        120|                125|                160|                100|                   0.34|\n",
                        "|1993-02-01|          4|Bay of Plenty Region|         435|        4851|         369|        140|                135|                160|                120|                   0.30|\n",
                        "|1993-02-01|          5|     Gisborne Region|          87|         780|          63|        120|                120|                140|                109|                   0.24|\n",
                        "|1993-02-01|          6|  Hawke's Bay Region|         237|        2967|         225|        140|                133|                155|                120|                   0.26|\n",
                        "|1993-02-01|          7|     Taranaki Region|         240|        2313|         183|        130|                128|                158|                108|                   0.36|\n",
                        "|1993-02-01|          8|Manawatu-Wanganui...|         807|        5484|         552|        140|                142|                200|                110|                   0.47|\n",
                        "|1993-02-01|          9|   Wellington Region|         975|       11376|         828|        170|                171|                213|                130|                   0.38|\n",
                        "|1993-02-01|         12|   West Coast Region|          24|         399|          24|        120|                 99|                130|                 80|                   0.37|\n",
                        "|1993-02-01|         13|   Canterbury Region|       1,242|       10731|         873|        150|                150|                190|                120|                   0.39|\n",
                        "|1993-02-01|         14|        Otago Region|         897|        5058|         567|        150|                149|                220|                120|                   0.56|\n",
                        "|1993-02-01|         15|    Southland Region|         150|        1434|          99|        100|                 92|                115|                 83|                   0.32|\n",
                        "|1993-02-01|         16|       Tasman Region|          33|         309|          30|        140|                125|                166|                110|                   0.43|\n",
                        "|1993-02-01|         17|       Nelson Region|         117|        1230|         117|        150|                145|                180|                129|                   0.26|\n",
                        "|1993-02-01|         18|  Marlborough Region|          51|         654|          54|        130|                123|                150|                110|                   0.20|\n",
                        "|1993-03-01|        -99|                 ALL|       7,812|       92724|       6,720|        150|                148|                180|                120|                   0.38|\n",
                        "|1993-03-01|         -1|                  NA|         360|        4932|         360|        130|                125|                160|                100|                   0.44|\n",
                        "+----------+-----------+--------------------+------------+------------+------------+-----------+-------------------+-------------------+-------------------+-----------------------+\n",
                        "only showing top 20 rows\n",
                        "\n",
                        "DataFrame schema:\n",
                        "root\n",
                        " |-- Time Frame: string (nullable = true)\n",
                        " |-- Location Id: string (nullable = true)\n",
                        " |-- Location: string (nullable = true)\n",
                        " |-- Lodged Bonds: string (nullable = true)\n",
                        " |-- Active Bonds: string (nullable = true)\n",
                        " |-- Closed Bonds: string (nullable = true)\n",
                        " |-- Median Rent: string (nullable = true)\n",
                        " |-- Geometric Mean Rent: string (nullable = true)\n",
                        " |-- Upper Quartile Rent: string (nullable = true)\n",
                        " |-- Lower Quartile Rent: string (nullable = true)\n",
                        " |-- Log Std Dev Weekly Rent: string (nullable = true)\n",
                        "\n"
                    ]
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
                        ],
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "bc539bf396ea4f418e9e052e8bf7c322"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "/usr/local/lib/python3.11/dist-packages/ydata_profiling/model/pandas/discretize_pandas.py:52: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 8 8 ... 9 9 9]' has dtype incompatible with int32, please explicitly cast to a compatible dtype first.\n",
                        "  discretized_df.loc[:, column] = self._discretize_column(\n",
                        "/usr/local/lib/python3.11/dist-packages/ydata_profiling/model/pandas/discretize_pandas.py:52: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[5 0 0 ... 0 0 0]' has dtype incompatible with int32, please explicitly cast to a compatible dtype first.\n",
                        "  discretized_df.loc[:, column] = self._discretize_column(\n",
                        "/usr/local/lib/python3.11/dist-packages/ydata_profiling/model/pandas/discretize_pandas.py:52: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[2 0 0 ... 0 0 0]' has dtype incompatible with int32, please explicitly cast to a compatible dtype first.\n",
                        "  discretized_df.loc[:, column] = self._discretize_column(\n",
                        "/usr/local/lib/python3.11/dist-packages/ydata_profiling/model/pandas/discretize_pandas.py:52: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[4 0 0 ... 0 0 0]' has dtype incompatible with int32, please explicitly cast to a compatible dtype first.\n",
                        "  discretized_df.loc[:, column] = self._discretize_column(\n",
                        "/usr/local/lib/python3.11/dist-packages/ydata_profiling/model/pandas/discretize_pandas.py:52: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1 0 0 ... 7 7 7]' has dtype incompatible with int32, please explicitly cast to a compatible dtype first.\n",
                        "  discretized_df.loc[:, column] = self._discretize_column(\n",
                        "/usr/local/lib/python3.11/dist-packages/ydata_profiling/model/pandas/discretize_pandas.py:52: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1 0 0 ... 7 7 6]' has dtype incompatible with int32, please explicitly cast to a compatible dtype first.\n",
                        "  discretized_df.loc[:, column] = self._discretize_column(\n",
                        "/usr/local/lib/python3.11/dist-packages/ydata_profiling/model/pandas/discretize_pandas.py:52: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1 0 0 ... 6 6 5]' has dtype incompatible with int32, please explicitly cast to a compatible dtype first.\n",
                        "  discretized_df.loc[:, column] = self._discretize_column(\n",
                        "/usr/local/lib/python3.11/dist-packages/ydata_profiling/model/pandas/discretize_pandas.py:52: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1 0 0 ... 6 6 7]' has dtype incompatible with int32, please explicitly cast to a compatible dtype first.\n",
                        "  discretized_df.loc[:, column] = self._discretize_column(\n",
                        "/usr/local/lib/python3.11/dist-packages/ydata_profiling/model/correlations.py:66: UserWarning: There was an attempt to calculate the auto correlation, but this failed.\n",
                        "To hide this warning, disable the calculation\n",
                        "(using `df.profile_report(correlations={\"auto\": {\"calculate\": False}})`\n",
                        "If this is problematic for your use case, please report this as an issue:\n",
                        "https://github.com/ydataai/ydata-profiling/issues\n",
                        "(include the error message: 'could not convert string to float: 'ALL'')\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
                        ],
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "e5f9fabaf06b4d9bb6bb391236d0f502"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
                        ],
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "58f4d8b5e02d4871901de69b82d1cc5f"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
                        ],
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "f4426fc8044c4e1ab06a20967d3bc0ae"
                        }
                    },
                    "metadata": {}
                }
            ],
            "source": [
                "# Create a SparkSession\n",
                "spark = SparkSession.builder.appName(\"ReadCSVLocal\").getOrCreate()\n",
                "\n",
                "# Define the schema explicitly\n",
                "# All columns are declared as StringType to load all and minimise parsing errors\n",
                "# due to mixed types or special values like \"-1\" or \"NA\" within quoted fields.\n",
                "# Casting occurs later\n",
                "schema = StructType([\n",
                "    StructField(\"Time Frame\", StringType(), True),\n",
                "    StructField(\"Location Id\", StringType(), True),\n",
                "    StructField(\"Location\", StringType(), True),\n",
                "    StructField(\"Lodged Bonds\", StringType(), True),\n",
                "    StructField(\"Active Bonds\", StringType(), True),\n",
                "    StructField(\"Closed Bonds\", StringType(), True),\n",
                "    StructField(\"Median Rent\", StringType(), True),\n",
                "    StructField(\"Geometric Mean Rent\", StringType(), True),\n",
                "    StructField(\"Upper Quartile Rent\", StringType(), True),\n",
                "    StructField(\"Lower Quartile Rent\", StringType(), True),\n",
                "    StructField(\"Log Std Dev Weekly Rent\", StringType(), True)\n",
                "])\n",
                "\n",
                "try:\n",
                "    # EXTRACTION PHASE - Check if the file exists\n",
                "    if not os.path.exists(fpath_tenancy_data):\n",
                "        print(f\"Error: File not found at {fpath_tenancy_data}\")\n",
                "    else:\n",
                "        print(f\"Attempting to read data from local file: {fpath_tenancy_data}\")\n",
                "        # EXTRACT file into a Spark DataFrame using the defined schema\n",
                "        df_spark = spark.read.csv(\n",
                "            f\"file://{fpath_tenancy_data}\",\n",
                "            header=True,\n",
                "            schema=schema,  # Use the explicitly defined schema\n",
                "            quote='\"',      # Specify that double quotes are used for quoting\n",
                "            escape='\"'      # Specify that double quotes are used for escaping\n",
                "        )\n",
                "\n",
                "        # Review the EXTRACTED DATA\n",
                "        print(\"DataFrame head:\")\n",
                "        df_spark.show()\n",
                "        print(\"DataFrame schema:\")\n",
                "        df_spark.printSchema()\n",
                "\n",
                "        # TRANSFORMATIONS\n",
                "        df_spark_tfm = fn_transform_cast(df_spark, [\"Time Frame\"], \"date\")\n",
                "        df_spark_tfm = fn_transform_cast(df_spark_tfm,\n",
                "         [\"Location Id\", \"Lodged Bonds\", \"Active Bonds\", \"Closed Bonds\",\n",
                "          \"Median Rent\", \"Upper Quartile Rent\", \"Lower Quartile Rent\",\n",
                "          \"Geometric Mean Rent\"], \"integer\")\n",
                "        df_spark_tfm = fn_transform_cast(df_spark_tfm, [\"Log Std Dev Weekly Rent\"], \"double\")\n",
                "\n",
                "        # Select both transformed and orginal fields to perform data quality reports on\n",
                "        selected_fields = [\"Time Frame\", \"tfm_Time Frame\",\n",
                "                         \"Location Id\", \"tfm_Location Id\",\n",
                "                         \"Location\",\n",
                "                         \"Lodged Bonds\", \"tfm_Lodged Bonds\",\n",
                "                         \"Active Bonds\", \"tfm_Active Bonds\",\n",
                "                         \"Closed Bonds\", \"tfm_Closed Bonds\",\n",
                "                         \"Median Rent\", \"tfm_Median Rent\",\n",
                "                         \"Geometric Mean Rent\", \"tfm_Geometric Mean Rent\",\n",
                "                         \"Upper Quartile Rent\", \"tfm_Upper Quartile Rent\",\n",
                "                         \"Lower Quartile Rent\", \"tfm_Lower Quartile Rent\",\n",
                "                         \"Log Std Dev Weekly Rent\", \"tfm_Log Std Dev Weekly Rent\"]\n",
                "        df_spark_tfm = fn_dataframe_selections(df_spark_tfm, selected_fields)\n",
                "\n",
                "        # - DATA QUALITY CHECK REPORT\n",
                "        # - Convert 'Time Frame' column to datetime objects in pandas\n",
                "        df_pandas = df_spark_tfm.toPandas()\n",
                "        df_pandas['tfm_Time Frame'] = pd.to_datetime(df_pandas['tfm_Time Frame'])\n",
                "        report = ProfileReport(df_pandas, title=\"Profiling pyspark DataFrame\")\n",
                "        # - Get the current date and time then save the Data Quality Report to github to eyeball (review manually)\n",
                "        now = datetime.datetime.now()\n",
                "        report.to_file(os.path.join(fpath_data_quality_profile, now.strftime(\"DataProfile_%Y%m%d_%H%M.html\")))\n",
                "\n",
                "except Exception as e:\n",
                "    print(f\"An error occurred during Spark processing: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "source": [
                "# 03 - Intermediate ETL of non-geospatial data structures\n",
                "- Dim tables for DATETIME (dim_period including new date/time attributes) and location (dim_location) created.\n",
                "- Fact table (fact_tenancy) is created"
            ],
            "metadata": {
                "id": "Mx2KSbwAfJNJ"
            }
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {
                "id": "-_KqgsZSvfwe",
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "outputId": "fe2ce33a-0a49-46eb-bb56-cf14a7d0a931"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "df_dim_location:\n",
                        "+---------------+--------------------+\n",
                        "|tfm_Location Id|            Location|\n",
                        "+---------------+--------------------+\n",
                        "|            -99|                 ALL|\n",
                        "|             -1|                  NA|\n",
                        "|              1|    Northland Region|\n",
                        "|              2|     Auckland Region|\n",
                        "|              3|      Waikato Region|\n",
                        "|              4|Bay of Plenty Region|\n",
                        "|              5|     Gisborne Region|\n",
                        "|              6|  Hawke's Bay Region|\n",
                        "|              7|     Taranaki Region|\n",
                        "|              8|Manawatu-Wanganui...|\n",
                        "|              9|   Wellington Region|\n",
                        "|             12|   West Coast Region|\n",
                        "|             13|   Canterbury Region|\n",
                        "|             14|        Otago Region|\n",
                        "|             15|    Southland Region|\n",
                        "|             16|       Tasman Region|\n",
                        "|             17|       Nelson Region|\n",
                        "|             18|  Marlborough Region|\n",
                        "+---------------+--------------------+\n",
                        "\n",
                        "root\n",
                        " |-- tfm_Location Id: integer (nullable = true)\n",
                        " |-- Location: string (nullable = true)\n",
                        "\n",
                        "\n",
                        "df_dim_period:\n",
                        "+--------------+----+--------------+-----------------+\n",
                        "|tfm_Time Frame|Year|Annual Quarter|Financial Quarter|\n",
                        "+--------------+----+--------------+-----------------+\n",
                        "|    1993-02-01|1993|        1993Q1|           1992Q3|\n",
                        "|    1993-03-01|1993|        1993Q1|           1992Q3|\n",
                        "|    1993-04-01|1993|        1993Q2|           1992Q4|\n",
                        "|    1993-05-01|1993|        1993Q2|           1992Q4|\n",
                        "|    1993-06-01|1993|        1993Q2|           1992Q4|\n",
                        "|    1993-07-01|1993|        1993Q3|           1993Q1|\n",
                        "|    1993-08-01|1993|        1993Q3|           1993Q1|\n",
                        "|    1993-09-01|1993|        1993Q3|           1993Q1|\n",
                        "|    1993-10-01|1993|        1993Q4|           1993Q2|\n",
                        "|    1993-11-01|1993|        1993Q4|           1993Q2|\n",
                        "|    1993-12-01|1993|        1993Q4|           1993Q2|\n",
                        "|    1994-01-01|1994|        1994Q1|           1993Q3|\n",
                        "|    1994-02-01|1994|        1994Q1|           1993Q3|\n",
                        "|    1994-03-01|1994|        1994Q1|           1993Q3|\n",
                        "|    1994-04-01|1994|        1994Q2|           1993Q4|\n",
                        "|    1994-05-01|1994|        1994Q2|           1993Q4|\n",
                        "|    1994-06-01|1994|        1994Q2|           1993Q4|\n",
                        "|    1994-07-01|1994|        1994Q3|           1994Q1|\n",
                        "|    1994-08-01|1994|        1994Q3|           1994Q1|\n",
                        "|    1994-09-01|1994|        1994Q3|           1994Q1|\n",
                        "+--------------+----+--------------+-----------------+\n",
                        "only showing top 20 rows\n",
                        "\n",
                        "root\n",
                        " |-- tfm_Time Frame: date (nullable = true)\n",
                        " |-- Year: integer (nullable = true)\n",
                        " |-- Annual Quarter: string (nullable = true)\n",
                        " |-- Financial Quarter: string (nullable = true)\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "df_facttenancyorder = [\n",
                "    \"tfm_Time Frame\",\n",
                "    \"tfm_Location Id\",\n",
                "    \"tfm_Lodged Bonds\",\n",
                "    \"tfm_Active Bonds\",\n",
                "    \"tfm_Closed Bonds\",\n",
                "    \"tfm_Median Rent\",\n",
                "    \"tfm_Geometric Mean Rent\",\n",
                "    \"tfm_Upper Quartile Rent\",\n",
                "    \"tfm_Lower Quartile Rent\",\n",
                "    \"tfm_Log Std Dev Weekly Rent\"\n",
                "    ]\n",
                "df_fact_tenancy = fn_dataframe_selections(df_spark_tfm, df_facttenancyorder)\n",
                "\n",
                "# Create df_dim_location\n",
                "location_cols = [\"tfm_Location Id\", \"Location\"]\n",
                "df_dim_location = fn_create_dim_table(df_spark_tfm, location_cols)\n",
                "\n",
                "# Show the location dimension\n",
                "print(\"df_dim_location:\")\n",
                "df_dim_location.show()\n",
                "df_dim_location.printSchema()\n",
                "\n",
                "# Create df_dim_period (basic) - original core of distinct time periods\n",
                "period_cols = [\"tfm_Time Frame\"]\n",
                "df_dim_period = fn_create_dim_table(df_spark_tfm, period_cols)\n",
                "df_dim_period = fn_add_period_attributes(df_dim_period, \"tfm_Time Frame\")\n",
                "\n",
                "# Show the period dimension with added attributes\n",
                "print(\"\\ndf_dim_period:\")\n",
                "df_dim_period.show()\n",
                "df_dim_period.printSchema()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "source": [
                "#04 - ETL of geospatial data structures\n",
                "- Quality checking and creation of geospatial data structure (df_dim_geospatial)\n",
                "- Joining of geospatial and location based data table (df_dim_locationV2)\n",
                "- Parquet files generated for all star schema tables (reasoning below):\n",
                "  - dim_location, dim_geospatial, dim_period, and fact_tenancy\n",
                "  - useful for time travel\n",
                "  - Supports both current and future/potential architectures (including icebergs, delta tables, AI development, and support for improved performance in application of analysis and data science methodologies).\n",
                "  - high compression and early initial transformations provide for better resource usage (i.e. processing, storage, memory)"
            ],
            "metadata": {
                "id": "ObCaZf3H4DSt"
            }
        },
        {
            "source": [
                "!pip install fiona\n",
                "!pip install shapely\n",
                "import fiona\n",
                "from shapely.geometry import shape\n",
                "\n",
                "fpath_data_geospatialdata = '/content/CommerceCommission202505/src_data/statsnz-regional-council-2023-clipped-generalised-SHP'\n",
                "data = []\n",
                "\n",
                "try:\n",
                "    with fiona.open(fpath_data_geospatialdata, 'r') as source:\n",
                "        for feature in source:\n",
                "            attributes = dict(feature['properties'])\n",
                "            geometry = shape(feature['geometry'])\n",
                "\n",
                "            # Option 1: Get centroid (for point-based maps in Power BI)\n",
                "            if geometry.geom_type in ['Point', 'Polygon', 'LineString', 'MultiPolygon', 'MultiLineString', 'MultiPoint']:\n",
                "                 # Check if geometry is valid before calculating centroid\n",
                "                 if geometry.is_valid:\n",
                "                    centroid = geometry.centroid\n",
                "                    attributes['centroid_longitude'] = centroid.x\n",
                "                    attributes['centroid_latitude'] = centroid.y\n",
                "                 else:\n",
                "                    print(f\"Warning: Invalid geometry found for feature {feature.get('id', 'unknown')}. Cannot calculate centroid.\")\n",
                "                    attributes['centroid_longitude'] = None\n",
                "                    attributes['centroid_latitude'] = None\n",
                "            else:\n",
                "                 attributes['centroid_longitude'] = None\n",
                "                 attributes['centroid_latitude'] = None\n",
                "\n",
                "            # Option 2: Convert to WKT (for use with custom Power BI visuals)\n",
                "            if geometry.is_valid:\n",
                "                attributes['geometry_wkt'] = geometry.wkt\n",
                "            else:\n",
                "                attributes['geometry_wkt'] = None # Assign None if geometry is invalid\n",
                "\n",
                "            data.append(attributes)\n",
                "\n",
                "    # Create Spark DataFrame and manipulate data\n",
                "    df_dim_geospatial = spark.createDataFrame(data)\n",
                "    df_dim_geospatial = fn_transform_cast(df_dim_geospatial, [\"REGC2023_V\"], \"integer\")\n",
                "    geospatial_cols = [\"tfm_REGC2023_V\", \"centroid_longitude\",  \"centroid_latitude\"]\n",
                "    df_dim_geospatial = fn_dataframe_selections(df_dim_geospatial, geospatial_cols)\n",
                "\n",
                "    # Join Geospatial data to Location data\n",
                "    df_dim_locationV2 = df_dim_location.join(df_dim_geospatial, col(\"tfm_Location Id\") == col(\"tfm_REGC2023_V\"), \"left\")\n",
                "    location_cols = [\"tfm_Location Id\", \"Location\", \"centroid_longitude\",  \"centroid_latitude\"]\n",
                "    df_dim_locationV2 = fn_dataframe_selections(df_dim_locationV2, location_cols).orderBy(\"tfm_Location Id\")\n",
                "\n",
                "    # Review datasets\n",
                "    df_dim_locationV2.show()\n",
                "    df_dim_locationV2.printSchema()\n",
                "\n",
                "    # Save to parquet files\n",
                "    df_fact_tenancy.write.parquet(os.path.join(fpath_data_star_schema, \"fact_tenancy.parquet\"), mode=\"overwrite\")\n",
                "    df_dim_locationV2.write.parquet(os.path.join(fpath_data_star_schema,\"dim_location.parquet\"), mode=\"overwrite\")\n",
                "    df_dim_period.write.parquet(os.path.join(fpath_data_star_schema, \"dim_period.parquet\"), mode=\"overwrite\")\n",
                "    df_dim_geospatial.write.parquet(os.path.join(fpath_data_star_schema, \"dim_geospatial.parquet\"), mode=\"overwrite\")\n",
                "\n",
                "except fiona.errors.DriverError as e:\n",
                "    print(f\"Error opening shapefile: {e}\")\n",
                "except Exception as e:\n",
                "    print(f\"An error occurred: {e}\")\n",
                "\n",
                "spark.stop()"
            ],
            "cell_type": "code",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "ejohfsgP9rxa",
                "outputId": "77bd8f5b-2b16-4d4a-bddc-4b506f440718"
            },
            "execution_count": 16,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Requirement already satisfied: fiona in /usr/local/lib/python3.11/dist-packages (1.10.1)\n",
                        "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/dist-packages (from fiona) (25.3.0)\n",
                        "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from fiona) (2025.4.26)\n",
                        "Requirement already satisfied: click~=8.0 in /usr/local/lib/python3.11/dist-packages (from fiona) (8.2.0)\n",
                        "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.11/dist-packages (from fiona) (1.1.1)\n",
                        "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.11/dist-packages (from fiona) (0.7.2)\n",
                        "Requirement already satisfied: shapely in /usr/local/lib/python3.11/dist-packages (2.1.0)\n",
                        "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.11/dist-packages (from shapely) (1.25.2)\n",
                        "+---------------+--------------------+------------------+-----------------+\n",
                        "|tfm_Location Id|            Location|centroid_longitude|centroid_latitude|\n",
                        "+---------------+--------------------+------------------+-----------------+\n",
                        "|            -99|                 ALL|              NULL|             NULL|\n",
                        "|             -1|                  NA|              NULL|             NULL|\n",
                        "|              1|    Northland Region|1674873.4819374217|6070435.718645753|\n",
                        "|              2|     Auckland Region|1754577.2159064335|5936226.622320191|\n",
                        "|              3|      Waikato Region|1819631.5961969332|5787806.489019746|\n",
                        "|              4|Bay of Plenty Region| 1935077.329116256|5769640.889597236|\n",
                        "|              5|     Gisborne Region|2030671.1605376513|5751619.397694061|\n",
                        "|              6|  Hawke's Bay Region|1923015.9375493263| 5633978.35972932|\n",
                        "|              7|     Taranaki Region|1723410.2936197936|5645242.690936126|\n",
                        "|              8|Manawatu-Wanganui...|1818923.0221697795|5597454.508396888|\n",
                        "|              9|   Wellington Region|1808336.0743669355|5448630.018855469|\n",
                        "|             12|   West Coast Region|1428664.9582552905|5260502.506577108|\n",
                        "|             13|   Canterbury Region|1484454.0504402276|5174472.342168632|\n",
                        "|             14|        Otago Region|1327793.9337584302|4982341.864956923|\n",
                        "|             15|    Southland Region| 1205194.330882515|4923950.529883005|\n",
                        "|             16|       Tasman Region|1570729.1080064573|5411572.279227861|\n",
                        "|             17|       Nelson Region|1632925.0380994014|5433824.730987117|\n",
                        "|             18|  Marlborough Region|1644330.1044643084|5384039.305067835|\n",
                        "+---------------+--------------------+------------------+-----------------+\n",
                        "\n",
                        "root\n",
                        " |-- tfm_Location Id: integer (nullable = true)\n",
                        " |-- Location: string (nullable = true)\n",
                        " |-- centroid_longitude: double (nullable = true)\n",
                        " |-- centroid_latitude: double (nullable = true)\n",
                        "\n"
                    ]
                }
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}